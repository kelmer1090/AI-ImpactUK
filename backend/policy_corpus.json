[
  {
    "id": "ICO-Audit DPIA-Requirement",
    "label": "ICO-Audit DPIA Requirement",
    "text": "Using AI to process personal data is likely to result in high risk, so you must complete a Data Protection Impact Assessment (DPIA) and, where residual high risk remains, consult the ICO before processing.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Accountability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Accountability-Documentation",
    "label": "ICO-Audit Accountability & Documentation",
    "text": "You should document and demonstrate how your AI system is compliant and be able to justify the choices you have made across its life-cycle.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Accountability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Controller-Processor-Determination",
    "label": "ICO-Audit Controller / Processor Determination",
    "text": "For each AI processing operation, you must determine whether you are a controller, joint controller or processor, and allocate corresponding responsibilities and contractual terms.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Accountability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Data-Governance-Framework",
    "label": "ICO-Audit Data Governance Framework",
    "text": "Put in place a data governance framework describing how all personal data used for training, testing or evaluating an AI system is correct, relevant, representative, complete and up to date.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Data Governance",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Access-Controls",
    "label": "ICO-Audit Access Controls & Segregation of Duties",
    "text": "Document access-management controls and segregation of duties so that changes to AI systems are made and signed off only by authorised personnel, with audit evidence retained.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Data Governance",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Statistical-Accuracy-Monitoring",
    "label": "ICO-Audit Statistical Accuracy Monitoring",
    "text": "From design onwards, establish accuracy metrics, adopt common terminology, and monitor model performance at a frequency proportionate to the potential harm of incorrect outputs, retraining as needed to address concept drift.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Accuracy",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Inference-Labeling",
    "label": "ICO-Audit Inference Labelling",
    "text": "Ensure records clearly mark AI outputs as statistically-informed guesses rather than facts, including provenance details of the data and model that generated each inference.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Transparency",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Concept-Drift",
    "label": "ICO-Audit Concept Drift Management",
    "text": "Define thresholds for acceptable performance degradation, detect drift and retrain the model on new data within timeframes appropriate to the use-case (e.g. every two years for dynamic recruitment criteria).",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Reliability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Pre-Deployment-Testing",
    "label": "ICO-Audit Pre-deployment Testing",
    "text": "Maintain a documented policy for pre-implementation testing and peer review of any AI system or significant change before go-live, retaining evidence of test results and approvals.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Robustness",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Supply-Chain-Security",
    "label": "ICO-Audit Supply-Chain Security",
    "text": "Assess security risks in externally-maintained code and frameworks, subscribe to vulnerability advisories, and enforce secure coding and source-code review processes.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Security/Resilience",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Isolated-Dev-Env",
    "label": "ICO-Audit Isolated Development Environment",
    "text": "Mitigate third-party code risks by developing ML models inside isolated virtual machines or containers, or by converting models into safer languages prior to deployment.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Security/Resilience",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Security-Risk-Assessment",
    "label": "ICO-Audit Security Risk Assessment",
    "text": "You must assess and manage AI-specific security risks – including adversarial attacks and data leakage – adopting measures proportionate to the nature, scope and context of the processing.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Security/Resilience",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Data-Minimisation-Techniques",
    "label": "ICO-Audit Data Minimisation Techniques",
    "text": "Where large datasets are required, apply privacy-preserving approaches (e.g., differential privacy, federated learning, synthetic data) to comply with the data minimisation principle while maintaining model utility.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Privacy",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Privacy-Attack-Mitigation",
    "label": "ICO-Audit Privacy Attack Mitigation",
    "text": "Evaluate susceptibility to membership inference, model inversion and other AI-specific privacy attacks, and put controls in place to prevent disclosure of training data via model outputs.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Privacy",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Black-Box-Limitations",
    "label": "ICO-Audit Black-Box Limitations",
    "text": "Deploy ‘black-box’ models only after assessing their impacts, ensuring your organisation can responsibly manage them, and supplementing them with interpretability tools that provide adequate explanations.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Explainability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Supplemental-Interpretability",
    "label": "ICO-Audit Supplemental Interpretability",
    "text": "Complex AI systems must include domain-appropriate interpretability techniques to support both individual explanations and internal oversight.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Interpretability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Algorithmic-Fairness-Monitoring",
    "label": "ICO-Audit Algorithmic Fairness Monitoring",
    "text": "Regularly monitor models for algorithmic fairness using appropriate statistical measures, documenting reviews and corrective actions.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Managing Bias",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Fairness-Constraints",
    "label": "ICO-Audit Fairness Constraints",
    "text": "Where discrimination is detected, add or remove data, retrain models with fairness constraints, or retrain designers to resolve biased performance.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Fairness",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Diversity-Attestation",
    "label": "ICO-Audit Diversity Attestation",
    "text": "Require documented approval and attestation to the diversity and representation of training and test data before it is used in an AI system.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Diversity & Inclusion",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Output-Context",
    "label": "ICO-Audit Output Context Transparency",
    "text": "Where AI outputs are subjective or probabilistic, records and downstream decisions must reflect those limitations to avoid misinterpreting them as factual.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Transparency",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Risk-Assessment",
    "label": "ICO-Audit Risk Assessment & Mitigation",
    "text": "Assess risks to individuals’ rights and freedoms whenever you design or deploy an AI system, implement proportional controls, and be prepared to halt deployment if risks cannot be managed.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Safety",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Article22-Classification",
    "label": "ICO-Audit Human Oversight & Article 22 Classification",
    "text": "Senior management must sign-off whether an AI system supports human decision-making or makes solely automated decisions, with clear lines of accountability and risk-management policies aligned to GDPR Article 22.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Accountability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Automation-Bias",
    "label": "ICO-Audit Automation Bias Controls",
    "text": "From the design stage, put controls in place to mitigate automation bias—eg, reviewer training, interpretability tools, and requirements that support meaningful human challenge of AI outputs.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Managing Bias",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Human-Review-Monitoring",
    "label": "ICO-Audit Human Review Monitoring",
    "text": "Continuously analyse how often human reviewers accept or reject AI outputs; if they routinely agree without genuine assessment, treat the process as solely automated and introduce additional safeguards or scrutiny.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Accountability",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Rights-AI-Outputs",
    "label": "ICO-Audit Individual Rights – AI Outputs",
    "text": "When AI predictions are stored in a user profile, they are subject to the rights of access, rectification and erasure; organisations must be able to correct or delete inaccurate outputs that affect individuals.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Privacy",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "ICO-Audit Rights-Training-Data",
    "label": "ICO-Audit Individual Rights – Training Data",
    "text": "You must consider and respond to access, rectification or erasure requests concerning personal data in training datasets, even if this necessitates re-training or deleting the model unless an Article 11 exemption applies.",
    "link": "https://ico.org.uk/media/about-the-ico/documents/2619990/guidance-on-the-ai-auditing-framework.pdf",
    "category": "Privacy",
    "document": "ICO AI Auditing Framework",
    "framework": "ICO"
  },
  {
    "id": "DSIT §3.2.3 Safety",
    "label": "DSIT §3.2.3 Safety, Security & Robustness",
    "text": "AI systems should function in a robust, secure and safe way throughout the AI life cycle, and risks should be continually identified, assessed and managed.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section-3-2-3",
    "category": "Safety",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT §3.2.3 Reliability",
    "label": "DSIT §3.2.3 Reliability",
    "text": "AI systems should be technically secure and should reliably function as intended and described.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section-3-2-3",
    "category": "Reliability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT §3.2.3 Transparency",
    "label": "DSIT §3.2.3 Transparency & Explainability",
    "text": "AI systems should be appropriately transparent and explainable.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section-3-2-3",
    "category": "Explainability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT §3.2.3 Fairness",
    "label": "DSIT §3.2.3 Fairness",
    "text": "AI systems should not undermine legal rights, discriminate unfairly against individuals or create unfair market outcomes.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section-3-2-3",
    "category": "Fairness",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT §3.2.3 Accountability",
    "label": "DSIT §3.2.3 Accountability & Governance",
    "text": "Governance measures should ensure effective oversight of AI systems, with clear lines of accountability established across the AI life cycle.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section-3-2-3",
    "category": "Accountability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT §3.2.3 Contestability",
    "label": "DSIT §3.2.3 Contestability & Redress",
    "text": "Users, impacted third parties and AI actors should be able to contest an AI decision or outcome that is harmful or creates a material risk of harm.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section-3-2-3",
    "category": "Accountability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Cybersecurity",
    "label": "Annex A - Cybersecurity guidance",
    "text": "Regulators should provide guidance on good cybersecurity practices (e.g. NCSC principles for securing machine-learning models).",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Security/Resilience",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Privacy",
    "label": "Annex A - Privacy safeguards",
    "text": "Guidance should cover privacy practices such as limiting access to authorised users and safeguarding against bad actors.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Privacy",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Risk-Management",
    "label": "Annex A - Risk-management framework",
    "text": "Regulators should refer AI life-cycle actors to an appropriate risk-management framework and require models to be reviewed over time.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Robustness",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Safety-Standards",
    "label": "Annex A - Safety & robustness standards",
    "text": "Regulators should consider technical standards (e.g. ISO/IEC 24029-2, 5259 series, TR 5469) to support implementation of risk treatment measures.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Robustness",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Info-Duties",
    "label": "Annex A - Information duties",
    "text": "AI actors must provide information on the system’s purpose, data (including training data), logic/process, and accountability for outcomes.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Transparency",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Explainability-Reqs",
    "label": "Annex A - Explainability requirements",
    "text": "Regulators should set explainability requirements – particularly for higher-risk systems – balancing enforcement needs and system robustness.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Explainability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Transparency-Standards",
    "label": "Annex A - Transparency standards",
    "text": "Regulators should reference standards such as IEEE 7001 and ISO/IEC TS 6254 to support transparency and explainability.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Explainability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Fairness-Define",
    "label": "Annex A - Define fairness",
    "text": "Regulators should interpret and articulate ‘fairness’ for their sector or domain and decide where it is relevant.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Fairness",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Fairness-Governance",
    "label": "Annex A - Fairness governance",
    "text": "Regulators should design, implement and enforce governance requirements for fairness applicable to regulated entities.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Fairness",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Legal-Justification",
    "label": "Annex A - Decision justification",
    "text": "Where AI decisions have a legal or similarly significant effect, regulators should consider requiring operators to provide appropriate justification to affected parties.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Managing Bias",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Vulnerable-Users",
    "label": "Annex A - Protect vulnerable users",
    "text": "AI systems should comply with rules protecting vulnerable individuals; regulators must consider how AI may alter or intensify user vulnerability.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Diversity & Inclusion",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Bias-Standards",
    "label": "Annex A - Bias-mitigation standards",
    "text": "Regulators should consider standards (e.g. ISO/IEC TR 24027, TR 24368) that address AI fairness and bias-mitigation.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Managing Bias",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Accountability-Assign",
    "label": "Annex A - Assign accountability",
    "text": "Regulators should determine who is accountable for compliance with existing regulation and the AI principles.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Accountability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Governance-Mechanisms",
    "label": "Annex A - Governance mechanisms",
    "text": "Regulators should provide guidance on governance mechanisms including risk-management and reporting processes.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Data Governance",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Governance-Standards",
    "label": "Annex A - Governance standards",
    "text": "Regulators should consider standards (e.g. ISO/IEC 23894, 42001) to maintain accountability within organisations.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Data Governance",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "DSIT AnnexA Redress-Guidance",
    "label": "Annex A - Routes to redress",
    "text": "Regulators should create or update guidance on where affected parties can direct complaints or disputes about AI harms.",
    "link": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a",
    "category": "Accountability",
    "document": "DSIT White Paper",
    "framework": "DSIT"
  },
  {
    "id": "ISO 42001 §4.1 Context",
    "label": "ISO 42001 §4.1 Understanding the organisation & its context",
    "text": "The organisation shall determine external and internal issues relevant to its purpose that affect its ability to achieve the intended result(s) of its AI management system (AIMS).",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §4.2 Interested-Parties",
    "label": "ISO 42001 §4.2 Needs & expectations of interested parties",
    "text": "The organisation shall determine the interested parties relevant to the AIMS and the requirements of those parties that are applicable to the AIMS.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §4.3 Scope",
    "label": "ISO 42001 §4.3 Scope of the AIMS",
    "text": "The organisation shall determine the boundaries and applicability of the AIMS, taking into account external & internal issues and interested-party requirements, and shall maintain documented information stating the scope.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §5.2 AI-Policy",
    "label": "ISO 42001 §5.2 AI Policy",
    "text": "Top management shall establish, implement and maintain an AI policy that is appropriate to the purpose of the organisation, includes a commitment to responsible development and use of AI systems and to continual improvement of the AIMS, and is communicated within the organisation and made available to interested parties, as appropriate.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Transparency",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §6.1 Risk-Opportunities",
    "label": "ISO 42001 §6.1 Actions to address risks & opportunities",
    "text": "The organisation shall determine AI-related risks and opportunities that need to be addressed to give assurance that the AIMS can achieve its intended result(s), prevent or reduce undesired impact(s) and achieve continual improvement.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Safety",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §6.2 AI-Objectives",
    "label": "ISO 42001 §6.2 AI Objectives & planning to achieve them",
    "text": "The organisation shall establish AI objectives at relevant functions and levels. Objectives shall be measurable (if practicable), monitored, communicated, updated and supported by plans that define responsible parties, resources, timelines and evaluation methods.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §7.2 Competence",
    "label": "ISO 42001 §7.2 Competence",
    "text": "The organisation shall determine the competence necessary for those doing work under its control that affects AI performance and reliability, ensure they are competent on the basis of appropriate education, training or experience, and retain documented information as evidence.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Diversity & Inclusion",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §7.3 Awareness",
    "label": "ISO 42001 §7.3 Awareness",
    "text": "Persons doing work under the organisation’s control shall be aware of the AI policy, their contribution to the effectiveness of the AIMS and the implications of not conforming to its requirements.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Transparency",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §7.5 Documented-Information",
    "label": "ISO 42001 §7.5 Documented information",
    "text": "The AIMS shall include documented information required by this Standard and as determined by the organisation for effectiveness and confidence. Documented information shall be controlled to ensure it is available, suitable and adequately protected.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §8.2 Data-Management",
    "label": "ISO 42001 §8.2 Data management",
    "text": "The organisation shall establish criteria and procedures to ensure data used in AI systems are relevant, representative, accurate, complete and up to date; data lineage shall be documented and quality monitored throughout the AI life cycle.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Data Governance",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §8.3 Design-Development",
    "label": "ISO 42001 §8.3 Design & development of AI systems",
    "text": "The organisation shall plan and control design and development of AI systems, including verification & validation activities to ensure outputs meet input requirements and intended use, with records of reviews and approvals maintained.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accuracy",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §8.5 Performance-Monitoring",
    "label": "ISO 42001 §8.5 Monitoring & measurement of AI performance",
    "text": "Performance of AI systems shall be monitored and measured against planned results at defined intervals; where necessary, corrective actions shall be taken to maintain required accuracy, robustness and reliability.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Reliability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §8.7 Incident-Management",
    "label": "ISO 42001 §8.7 Incident & change management",
    "text": "The organisation shall establish processes to identify, report, investigate and act on AI-related incidents or non-conformities, including security or safety issues, and control changes to AI systems to prevent unintended consequences.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Security/Resilience",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §9.2 Internal-Audit",
    "label": "ISO 42001 §9.2 Internal audit",
    "text": "The organisation shall conduct internal audits at planned intervals to determine whether the AIMS conforms to the requirements of this Standard and is effectively implemented and maintained.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 §10.1 Corrective-Action",
    "label": "ISO 42001 §10.1 Nonconformity & corrective action",
    "text": "When a non-conformity occurs, the organisation shall react to it, evaluate the need for action to eliminate the cause, implement any action needed, review the effectiveness and make changes to the AIMS if necessary.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.2 Data-Quality",
    "label": "ISO 42001 Annex A 6.2 Data quality",
    "text": "Controls shall be in place to ensure data used for AI is of sufficient quality, including relevance, representativeness, accuracy, completeness, consistency and timeliness, with responsibilities assigned for data quality assurance.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accuracy",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.5 Robustness-Accuracy",
    "label": "ISO 42001 Annex A 6.5 Robustness & accuracy of AI systems",
    "text": "The organisation shall implement controls to detect, prevent and mitigate errors, faults and unexpected behaviour in AI systems, ensuring robust performance across the intended operating range.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Robustness",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.6 Bias-Fairness",
    "label": "ISO 42001 Annex A 6.6 Bias & fairness",
    "text": "Procedures shall exist to identify, assess and mitigate bias in data, models and outcomes, supporting fair and non-discriminatory AI performance.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Managing Bias",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.7 Security",
    "label": "ISO 42001 Annex A 6.7 Security of AI systems",
    "text": "Security controls shall protect AI systems and assets against threats including adversarial manipulation, model extraction, data poisoning and privacy attacks, maintaining confidentiality, integrity and availability.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Security/Resilience",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.8 Explainability",
    "label": "ISO 42001 Annex A 6.8 Explainability & interpretability",
    "text": "The organisation shall ensure that explanations of AI system behaviour are available, appropriate for the context and audience, and provide sufficient information for users to understand outputs.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Explainability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.9 Transparency",
    "label": "ISO 42001 Annex A 6.9 Transparency & traceability",
    "text": "Traceability of data, models and decisions shall be maintained; relevant information about the AI system’s purpose, limitations and performance shall be made available to stakeholders.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Transparency",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.10 Privacy",
    "label": "ISO 42001 Annex A 6.10 Privacy & data protection",
    "text": "AI development and operation shall comply with applicable data-protection regulations, applying privacy-by-design principles and minimising personal data processing.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Privacy",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.11 Safety-Human-Oversight",
    "label": "ISO 42001 Annex A 6.11 Safety & human oversight",
    "text": "AI systems shall include appropriate human oversight mechanisms to prevent or mitigate harms and enable intervention when necessary.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Safety",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.12 Accountability-Roles",
    "label": "ISO 42001 Annex A 6.12 Accountability & roles",
    "text": "Roles and responsibilities for AI governance, risk management and compliance shall be defined, assigned and communicated across the organisation.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Accountability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.6.13 Diversity-Inclusion",
    "label": "ISO 42001 Annex A 6.13 Diversity & inclusion",
    "text": "The organisation shall consider diversity and inclusion throughout the AI life cycle, engaging a diverse range of stakeholders and avoiding exclusionary impacts.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Diversity & Inclusion",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.7 Impact-Assessment",
    "label": "ISO 42001 Annex A 7 AI system impact assessment",
    "text": "A formal, documented AI system impact assessment shall be performed to identify, evaluate and address impacts on individuals, groups or society arising from AI use.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Safety",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.8 Data-Governance",
    "label": "ISO 42001 Annex A 8 Data governance",
    "text": "Controls shall govern data collection, storage, processing and deletion, ensuring compliance with data-quality requirements and lifecycle policies.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Data Governance",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.9 Design-Controls",
    "label": "ISO 42001 Annex A 9 Design & development controls",
    "text": "Design and development of AI systems shall follow documented processes that incorporate risk assessment, requirements traceability and validation against user needs.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Robustness",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.10 Lifecycle-Operations",
    "label": "ISO 42001 Annex A 10 AI life-cycle operations",
    "text": "Operational controls shall cover deployment, monitoring, maintenance and retirement of AI systems to ensure ongoing reliability and alignment with intended purpose.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Reliability",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.11 Incident-Response",
    "label": "ISO 42001 Annex A 11 Incident response & remediation",
    "text": "The organisation shall establish and maintain processes for timely detection, reporting and remediation of AI-related incidents, with lessons learned feeding back into continual improvement.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Security/Resilience",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  },
  {
    "id": "ISO 42001 AnnexA A.12 Transparency-Info",
    "label": "ISO 42001 Annex A 12 Information for transparency",
    "text": "Information provided to users and stakeholders shall include the AI system’s purpose, capabilities, limitations, performance metrics and any significant risks, enabling informed decision-making.",
    "link": "https://www.iso.org/standard/81259.html",
    "category": "Transparency",
    "document": "ISO/IEC 42001:2023",
    "framework": "ISO"
  }
]